{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) involves a series of preprocessing steps to transform raw text data into a format suitable for analysis or machine learning models. These steps help improve the quality of the data and make it easier for algorithms to understand and process the text. Below are the key preprocessing steps used in NLP, along with explanations and example code."
      ],
      "metadata": {
        "id": "VA8FcfSl5Ifo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Lowercasing\n",
        "Convert all text to lowercase to ensure uniformity and avoid treating the same words in different cases as different tokens."
      ],
      "metadata": {
        "id": "jTBEgBIx5KAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello World! This is NLP.\"\n",
        "text = text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibxeXdSX5QmQ",
        "outputId": "797c8b6c-2700-47f5-bd5b-e42624aa6ff4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world! this is nlp.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tokenization\n",
        "Split the text into individual words or tokens. This is a fundamental step in NLP."
      ],
      "metadata": {
        "id": "sgzINJkj5j6A"
      }
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello World! This is NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp3V8UTn7Pzj",
        "outputId": "92df7d3b-f661-43c1-8aa2-7289517d8c12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'World', '!', 'This', 'is', 'NLP', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Removing Punctuation\n",
        "Punctuation marks are often unnecessary for analysis and can be removed."
      ],
      "metadata": {
        "id": "g7pzvinK77qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello, World! This is NLP.\"\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VgSi-F77-vC",
        "outputId": "26a5d352-ca4a-4197-dc04-b5867d1707d8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World This is NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Removing Stopwords\n",
        "Stopwords are common words (e.g., \"the\", \"is\", \"and\") that do not contribute much to the meaning of the text. Removing them can reduce noise."
      ],
      "metadata": {
        "id": "7nLivAd98Eh5"
      }
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'stopwords' dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)  # Output: ['sample', 'sentence']"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXB4lrR68RX4",
        "outputId": "423ced74-27e3-4c66-d16a-fc44a32e07f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Stemming\n",
        "Stemming reduces words to their root form by removing suffixes. It may not always result in a valid word."
      ],
      "metadata": {
        "id": "YZtOCfn68ZaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runner\", \"ran\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)  # Output: ['run', 'runner', 'ran']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J3CgwiW8csR",
        "outputId": "78db01ac-bf6f-4117-ec2e-04326ef1341d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'ran']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Lemmatization\n",
        "Lemmatization reduces words to their base or dictionary form (lemma). Unlike stemming, it ensures the result is a valid word."
      ],
      "metadata": {
        "id": "KLLaMpZ88mVh"
      }
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"runner\", \"ran\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(lemmatized_words)  # Output: ['run', 'run', 'run']"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fxqECzE8yIu",
        "outputId": "6b56d7a1-c7a1-45e2-ce98-7182eadd5cb9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'run']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Removing Numbers\n",
        "Numbers may not be relevant for text analysis and can be removed."
      ],
      "metadata": {
        "id": "lvOiRRta83qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"There are 3 apples and 5 oranges.\"\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "print(text)  # Output: \"There are  apples and  oranges.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4XYcCa885pI",
        "outputId": "0f233068-b24e-4eca-a282-f1b00b72204e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  apples and  oranges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Removing Extra Spaces\n",
        "Extra spaces can be removed to clean up the text."
      ],
      "metadata": {
        "id": "GhHOYpnc8_a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"   This   is   a   sentence.   \"\n",
        "text = ' '.join(text.split())\n",
        "print(text)  # Output: \"This is a sentence.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p708ebx88-1b",
        "outputId": "8aa19831-fbb4-4bd8-9246-3b0906c804a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Handling Contractions\n",
        "Expand contractions (e.g., \"can't\" ‚Üí \"cannot\") to standardize the text."
      ],
      "metadata": {
        "id": "HXVy7Tbn9GlH"
      }
    },
    {
      "source": [
        "!pip install contractions\n",
        "from contractions import fix\n",
        "\n",
        "text = \"I can't do this.\"\n",
        "text = fix(text)\n",
        "print(text)  # Output: \"I cannot do this.\""
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeP41nxl9O4n",
        "outputId": "c3d5e2d9-aed6-49ab-9085-1cd53b0935e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "I cannot do this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Removing Special Characters\n",
        "Special characters (e.g., @, #, $) can be removed if they are not relevant."
      ],
      "metadata": {
        "id": "Nopfcg3o9a14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"This is a #sample text with @special characters!\"\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(text)  # Output: \"This is a sample text with special characters\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmd6PsKi9fvZ",
        "outputId": "a627ca8c-ebc8-4394-d8ff-9d3fa2eb35fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample text with special characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Part-of-Speech (POS) Tagging\n",
        "Assign parts of speech (e.g., noun, verb) to each word in the text."
      ],
      "metadata": {
        "id": "VwIEplxz9i8v"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the required resource\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "tokens = word_tokenize(\"This is a sample sentence.\")\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)  # Output: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fQ0rflm9sQ1",
        "outputId": "3fc11f3d-500f-41a4-8979-5f8ebf4840ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Named Entity Recognition (NER)\n",
        "Identify and classify named entities (e.g., names, dates, locations) in the text."
      ],
      "metadata": {
        "id": "CwJjTxSq9xV9"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the required resources\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Download the 'maxent_ne_chunker_tab' resource\n",
        "nltk.download('maxent_ne_chunker_tab') # This line is crucial to fix the error.\n",
        "\n",
        "tokens = word_tokenize(\"John works at Google in New York.\")\n",
        "pos_tags = pos_tag(tokens)\n",
        "ner_tags = ne_chunk(pos_tags)\n",
        "print(ner_tags)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbiwZ-03_Oim",
        "outputId": "26a8ee33-80f6-4c95-8b7f-e3001255d1c4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON John/NNP)\n",
            "  works/VBZ\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  in/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Vectorization\n",
        "Convert text into numerical representations (e.g., Bag of Words, TF-IDF, Word Embeddings) for machine learning models."
      ],
      "metadata": {
        "id": "5L4lt8gk-fMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())  # Output: [[1 1 1 1 0], [0 1 1 0 1]]\n",
        "print(vectorizer.get_feature_names_out())  # Output: ['another', 'example', 'is', 'sample', 'sentence', 'this']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaZi-n0I-iBN",
        "outputId": "7a75c491-b26f-4597-ccf3-d4d53bcab2b4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1 1 1]\n",
            " [1 1 0 0 1 0]]\n",
            "['another' 'example' 'is' 'sample' 'sentence' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Handling Missing Data\n",
        "If the dataset contains missing text, it can be filled or removed."
      ],
      "metadata": {
        "id": "8lIlehd7-m8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\"text\": [\"Hello\", None, \"World\"]}\n",
        "df = pd.DataFrame(data)\n",
        "df[\"text\"].fillna(\"My Dear\", inplace=True)  # Fill missing values\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-6V2lRg-pKE",
        "outputId": "e59f850b-1b74-4fde-c904-d08cf3a98c69"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      text\n",
            "0    Hello\n",
            "1  My Dear\n",
            "2    World\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-5f26b67bc58a>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"text\"].fillna(\"My Dear\", inplace=True)  # Fill missing values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Normalization\n",
        "Normalize text by converting it to a standard format (e.g., Unicode normalization)."
      ],
      "metadata": {
        "id": "DQvW0bvL-5Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "text = \"Caf√©\"\n",
        "text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "print(text)  # Output: \"Cafe\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSvirtFL-6_j",
        "outputId": "6c2e6943-6aec-4345-a0cf-bd72ea72d230"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cafe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. Spelling Correction\n",
        "Correct spelling errors in the text to improve consistency."
      ],
      "metadata": {
        "id": "EyCXabpZAY_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I made a many mistakes in Artificial intellengence\"\n",
        "blob = TextBlob(text)\n",
        "corrected_text = blob.correct()\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w0cwqtYAbLo",
        "outputId": "b5914870-c075-4ad3-b077-af9212ed4a4e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I made a many mistakes in Artificial intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17. Handling Emojis and Emoticons\n",
        "Convert emojis and emoticons to text or remove them, depending on the use case."
      ],
      "metadata": {
        "id": "e3VYfHddBDuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "\n",
        "import emoji\n",
        "\n",
        "text = \"I love Python! üòä\"\n",
        "# Convert emojis to text\n",
        "text = emoji.demojize(text)\n",
        "print(text)  # Output: \"I love Python! :smiling_face_with_smiling_eyes:\"\n",
        "\n",
        "# Remove emojis\n",
        "text = emoji.replace_emoji(text, replace=\"\")\n",
        "print(text)  # Output: \"I love Python! \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw9S3e2VB5rV",
        "outputId": "c1c3dd6e-00e9-4833-e403-61b63796a0d1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m409.6/590.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "I love Python! :smiling_face_with_smiling_eyes:\n",
            "I love Python! :smiling_face_with_smiling_eyes:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. Removing HTML Tags\n",
        "If the text contains HTML tags, they can be removed."
      ],
      "metadata": {
        "id": "r7m1IGJ0Bb5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "text = \"<p>This is a <b>sample</b> text.</p>\"\n",
        "soup = BeautifulSoup(text, \"html.parser\")\n",
        "clean_text = soup.get_text()\n",
        "print(clean_text)  # Output: \"This is a sample text.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sd2JEorBgwx",
        "outputId": "0a1697fa-4777-4a4d-f8ff-165cbb696239"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sample text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 19. Handling URLs\n",
        "Remove or replace URLs in the text."
      ],
      "metadata": {
        "id": "cOd5sGaoCVv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Visit my website at https://example.com.\"\n",
        "text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "print(text)  # Output: \"Visit my website at .\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cwxBUP0CYfb",
        "outputId": "1025499f-43a9-497a-a7b1-fd1662e278a3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visit my website at \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. Handling Mentions and Hashtags\n",
        "Remove or process mentions (e.g., @username) and hashtags (e.g., #NLP) in social media text.\n",
        "\n"
      ],
      "metadata": {
        "id": "icBnUhEdCeJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hey @user, check out #NLP!\"\n",
        "text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "print(text)  # Output: \"Hey , check out !\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_k8SLdHCgoD",
        "outputId": "620b7ed7-bc71-47af-f04d-13a12e441563"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey , check out !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. Sentence Segmentation\n",
        "Split a paragraph into individual sentences."
      ],
      "metadata": {
        "id": "yyy1nLmUClQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"This is the first sentence. This is the second sentence.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)  # Output: ['This is the first sentence.', 'This is the second sentence.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S2gaRPJCn4b",
        "outputId": "1f4b72d7-c1bb-4b0a-af34-407c5960ff69"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.', 'This is the second sentence.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. Handling Abbreviations\n",
        "Expand abbreviations to their full forms for better understanding."
      ],
      "metadata": {
        "id": "wkDSqmyKCuP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "\n",
        "import contractions\n",
        "\n",
        "text = \"I'll be there ASAP.\"\n",
        "expanded_text = contractions.fix(text)\n",
        "print(expanded_text)  # Output: \"I will be there as soon as possible.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AALr8lrUD-sX",
        "outputId": "21be5ac2-02a3-4f1c-9d30-5cd5ed13c638"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "I will be there AS SOON AS POSSIBLE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. Language Detection\n",
        "Identify the language of the text and filter out non-relevant languages.\n",
        "\n"
      ],
      "metadata": {
        "id": "QzxGYv3dFlVv"
      }
    },
    {
      "source": [
        "!pip install langdetect\n",
        "\n",
        "from langdetect import detect\n",
        "\n",
        "text = \"Ceci est un texte en fran√ßais.\"\n",
        "language = detect(text)\n",
        "print(language)  # Output: \"fr\""
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lH4OZz3F3bK",
        "outputId": "ce67518e-4f30-444e-d328-25b62a69e03a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=bbce2c6af92708087977e1fd3c221df7ecadf85a3428e8d49ecee05dc16a7b63\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. Text Encoding\n",
        "Ensure the text is in a consistent encoding format (e.g., UTF-8)."
      ],
      "metadata": {
        "id": "QpFyTwj1GRzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Caf√©\"\n",
        "text = text.encode('utf-8').decode('utf-8')\n",
        "print(text)  # Output: \"Caf√©\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQeARJXsGUz6",
        "outputId": "f3231ab3-caac-4b8c-d849-9a8df18fd909"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caf√©\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25. Handling Whitespace Tokens\n",
        "Remove tokens that are purely whitespace."
      ],
      "metadata": {
        "id": "P5eMzW91Gd-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"This\", \" \", \"is\", \" \", \"a\", \" \", \"sample\", \" \"]\n",
        "tokens = [token for token in tokens if token.strip()]\n",
        "print(tokens)  # Output: ['This', 'is', 'a', 'sample']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrV9J0leGiRp",
        "outputId": "f344e86d-a59e-46f0-884d-69fa303337dc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 26. Handling Dates and Times\n",
        "Normalize or remove dates and times from the text."
      ],
      "metadata": {
        "id": "bRRpjgkaIZ0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dateutil.parser as dparser\n",
        "\n",
        "text = \"The event is on 2023-10-15.\"\n",
        "date = dparser.parse(text, fuzzy=True)\n",
        "print(date)  # Output: 2023-10-15 00:00:00"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2L8tAGjIdWn",
        "outputId": "f0219e3a-1fb0-4825-d278-52cc86e446df"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-15 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 27. Text Augmentation\n",
        "Generate variations of the text to increase dataset size (useful for training)."
      ],
      "metadata": {
        "id": "84-G_uRiIiit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOq3-zpFIsUO",
        "outputId": "6a0e946b-fe4d-4ecc-9957-3dc6f2c61950"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nlpaug # Install the nlpaug library\n",
        "from nlpaug.augmenter.word import SynonymAug\n",
        "\n",
        "aug = SynonymAug(aug_src='wordnet')\n",
        "text = \"This is a sample text.\"\n",
        "augmented_text = aug.augment(text)\n",
        "print(augmented_text)  # Output: \"This is an example text.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m09PU8VgIvZG",
        "outputId": "af9a816d-05f9-4e5e-c9a9-52605973ad17"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a sample schoolbook.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28. Handling Negations\n",
        "Detect and handle negations (e.g., \"not good\" ‚Üí \"not_good\") to preserve meaning."
      ],
      "metadata": {
        "id": "RV-wzDMqJGDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "text = \"This is not good.\"\n",
        "tokens = word_tokenize(text)\n",
        "for i, token in enumerate(tokens):\n",
        "    if token == \"not\" and i + 1 < len(tokens):\n",
        "        tokens[i + 1] = \"not_\" + tokens[i + 1]\n",
        "print(tokens)  # Output: ['This', 'is', 'not', 'not_good', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zltbg2phJKtF",
        "outputId": "f289d8e2-9468-4da0-e415-8dd73990d651"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'not', 'not_good', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#29. Dependency Parsing\n",
        "Analyze the grammatical structure of a sentence."
      ],
      "metadata": {
        "id": "IxH1j9qzJPxV"
      }
    },
    {
      "source": [
        "import spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm # Download the model if not already downloaded\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the model directly using spacy.load\n",
        "\n",
        "# The rest of your code remains the same\n",
        "text = \"This is a sample sentence.\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sknfsnSSJ3nJ",
        "outputId": "e46ede33-68ca-4d8a-e5cb-4b60cc80c6a2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "This nsubj is\n",
            "is ROOT is\n",
            "a det sentence\n",
            "sample compound sentence\n",
            "sentence attr is\n",
            ". punct is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o8P8Wir6KNxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30. Handling Rare Words\n",
        "Replace rare words with a special token (e.g., <UNK>) to reduce vocabulary size."
      ],
      "metadata": {
        "id": "4gh8WuekKN0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "tokens = [\"this\", \"is\", \"a\", \"rare\", \"word\", \"word\"]\n",
        "word_counts = Counter(tokens)\n",
        "rare_words = {word for word, count in word_counts.items() if count < 2}\n",
        "tokens = [token if token not in rare_words else \"<UNK>\" for token in tokens]\n",
        "print(tokens)  # Output: ['this', 'is', 'a', '<UNK>', 'word', 'word']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8o7CbztKVSA",
        "outputId": "a119fe8a-d99b-40fd-8387-1d0a9ca12ea2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<UNK>', '<UNK>', '<UNK>', '<UNK>', 'word', 'word']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 31. Text Chunking\n",
        "Group words into \"chunks\" based on their POS tags.\n",
        "\n"
      ],
      "metadata": {
        "id": "47tijjJkKhur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "text = \"This is a sample sentence.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10qzm4zrKkOJ",
        "outputId": "022b4f2d-77b4-4a01-8328-55b4993f00a8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S This/DT is/VBZ (NP a/DT sample/JJ sentence/NN) ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 32. Handling Synonyms\n",
        "Replace words with their synonyms to reduce redundancy."
      ],
      "metadata": {
        "id": "18Ep4ouvKpnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "word = \"happy\"\n",
        "synonyms = wordnet.synsets(word)\n",
        "print([syn.lemmas()[0].name() for syn in synonyms])  # Output: ['happy', 'felicitous', 'glad', 'well', 'content', ...]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kva_NqbWKsJS",
        "outputId": "9e73ec45-b690-4169-f137-123ba5d6d953"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['happy', 'felicitous', 'glad', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 33. Text Normalization for Social Media\n",
        "Normalize social media text (e.g., \"loooove\" ‚Üí \"love\")."
      ],
      "metadata": {
        "id": "UL3eZtESKvw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"I loooove this!\"\n",
        "text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "print(text)  # Output: \"I love this!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CCVtabEKz7B",
        "outputId": "28c60784-18df-4d9a-9bd3-ee19d9b65b0f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love this!\n"
          ]
        }
      ]
    }
  ]
}